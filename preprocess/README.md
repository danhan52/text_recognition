## Preprocessing for text recognition

Each of the files in this folder except preprocess_all.py is meant to set up the data for ingestion into the text recognition model. I'll first describe how to use preprocess_all.py then go over the individual files for the different datasets. Finally, I'll describe the general output of the preprocessing functions in order to add more datasets as time goes on.

### File descriptions

#### preprocess_all.py
This will do all the preprocessing work needed to run the models found in modeling on any of the datasets that I have previously used. It has two optional arguments, the first is \<preprocess\_which\> which must be one of "all", "bentham", "iam", "combined", or "asm" signifying which dataset to preprocess. The second argument is a float \<resize\_to\> which specifies the fraction of the original image to resize the images to e.g. 0.5 means half the original image size. This script copies the "alphabet.txt" and "img_size.txt" files to ~/data which is necessary to run the models in ~/modeling. The defaults for \<preprocess\_which\> and \<resize\_to\> are "all" and "0.5", respectively.

#### preprocess_bentham.py, preprocess_iam.py, and preprocess_combined.py
These scripts preprocess the Bentham, IAM, and combined Bentham/IAM datasets, respectively. They can be run from the command line with "python preprocess\_\<dataset\>.py \<training\> \<resize\_to\> \<print\_letters\>". The arguments are optional with defaults of "both", "0.5", and "False", respectively. \<training\> can be one of "train", "test", or "both", which respectively correspond to preprocessing the training set, the test set, or both. \<resize\_to\> is a float specifying the fraction of the original image to resize the images to e.g. 0.5 means half the original image size. \<print\_letters\> is a boolean signal whether to print the letter frequencies within the data. If it is False (the default), it will print the number of letters in the dataset. Running any of these scripts will not add "alphabet.txt" and "img_size.txt" to ~/data which is necessary to run the models in ~/modeling which is why I recommend running preprocess_all.py for actual set up and using these files to guide creation of new preprocessing files.

#### preprocess_ASM_csv.py
This script still has some work to go before it's in its final stage because right now it's terribly slow. It takes the the Anti-Slavery Manuscripts' classification export and subject export and converts them into a long csv with a single row per line that a volunteer transcribed, in the order the transcriptions occurred. This helps feed into create_ASM_batch.py during the online training process. At the moment, this function only has one argument which is \<print\_letters\>, a boolean signal whether to print the letter frequencies within the data.

#### create_ASM_batch.py
This script creates the same thing as the preprocess\_\<dataset\>.py files above, but does so on a portion of the Anti-Slavery Manuscripts data, output by preprocess_ASM_csv.py. To run it, use "python create_ASM_batch.py \<batch\_end\> \<batch\_size\> \<resize\_to\> \<rand\_batch\>". \<batch\_end\> is the location of the end of the current batch of data, e.g. 10,000 means take from the first 10,000 volunteer transcriptions. \<batch\_size\> is the size of the batch to be created. \<resize\_to\> is a float specifying the fraction of the original image to resize the images to e.g. 0.5 means half the original image size. \<rand\_batch\> is a boolean indicating whether to do a random batch or an ordered batch (True is random batch). For a random batch in will randomly select \<batch\_size\> items from the first \<batch\_end\> transcriptions; for an ordered batch, it will select transcriptions \<batch\_end\>-\<batch\_size\> to \<batch\_end\>.

### Preprocessing output
There are four pieces output by the preprocessing that is necessary for modeling.  
First, is an alphabet file, alphabet.txt, which contains a one line string with all characters found in all transcriptions for the dataset you'll be working with. This must end up in ~/data for the code in ~/modeling to work, which is done automatically by preprocess_all.py.  
Second, is a image size file, img_size.txt, which contains a string of the format "w, h" where w is the width of the largest image and h in the height of the largest image. This is needed to create the size of the input to the model. Similar to alphabet.txt, this must end up in ~/data for the code in ~/modeling to work, which is done automatically by preprocess_all.py.  
Third, are the images that the model will be trained on. There is not specific form the images must be in, though smaller images will train faster than large ones, hence the option to resize the images in the preprocessing files. The images will go into a folder in the dataset folder, e.g. ~/data/BenthamDataset/Images_mod.  
Fourth, is a csv file located in the dataset folder named train.csv, e.g. ~/data/BenthamDataset/train.csv. This csv has two columns: "new_img_path" which contains the **FULL PATH** to the images and "transcription" which contains the transcriptions. While it's saved as a csv, it's technically a tsv so make sure to use tab separation and not comma separation.
